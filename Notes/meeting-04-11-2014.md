Compare against

1) Different acc, each has separate scratchpad - CPU will coordinate data flow.
Move loads up, run prefetches on CPU side and then run the same on the ACC size.
Fake extra CPU as DMA controller with low link latency.

Load hoisting depends on the size of the fully associative scratchpad only. (no instruction dependencies)

Motivate using data movement between functions

2) Super small L1 (1 block if possible), L2 is 3-4 cycles away. No pre-processing approach. This will simulate CODA.

For these 2 approaches, lease time needs to be set properly. Set the lease time to the acc latency left (make sure its not a really long time).


-------------- Our systems

3) Sweep different L1 sizes and L2 sizes, writes are write through, reads are cached.
4) How to pipeline? Issues loads from the 2nd acc block side, but don't account for them in the stats. Either do this by modifying the source to pipeline, or push the accesses into the other sequencer. 
